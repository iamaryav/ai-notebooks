{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning / Neural Network - Intuition\n",
    "Neuron - Mathematical equation based on some input\n",
    "Layer - collection of Neuron\n",
    "Activations - Output of the Neuron\n",
    "input layer\n",
    "- Neuron takes all the input and tries to ignore which is not relevant talking about single neuron\n",
    "- It should be based on the equation and the parameter tied to it\n",
    "\n",
    "Input layer vector goes to neuron layer then it produces the another vector then it is input for\n",
    "next layer and the last layer produces the output\n",
    "\n",
    "\n",
    "Input Layer -> Hidden Layer -> Output layer\n",
    "Just logistics regression but it learns from it features and create an another set of features\n",
    "and derives output from that\n",
    "- It creates it own set of new features that is the good thing about it or bad thing about it\n",
    "\n",
    "We need to decide:\n",
    "How many hidden layers we want\n",
    "How many neurons in each layer we want\n",
    "Called Neural network architecture\n",
    "\n",
    "Multiple layer of neural networks called - Multilayer Perceptron\n",
    "\n",
    "Face Recognition Example:\n",
    "Neurons layer distribute their work to do some useful prediction\n",
    "Fascinating thing is that it does by itself No one told to do so\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layers\n",
    "every neurons is logistics regression takes some input and and produces some probablity\n",
    "every layers take input from previous layer output and produces the output that becomes input\n",
    "of the next layer\n",
    "Hidden Layer - collection of neurons\n",
    "Hidden Units - Neurons\n",
    "Layer type - Dense\n",
    "Dense layer is called dense because of its connection with all the previous and next node\n",
    "\n",
    "#### Complex Neural networks\n",
    "\n",
    "#### Inference Forward propagation (making prediction)\n",
    "computation goes to left to right \n",
    "propagating the activation from left to right\n",
    "\n",
    "- Try the Inference of the multiple models by downloading and running them on your machine\n",
    "- Will use pytorch for doing the above and for learning purpose\n",
    "- Tensor, Matmul\n",
    "- Matrix multiplication, dot product\n",
    "- a dot b or a ^ T dot b\n",
    "- vectorize Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch as torch\n",
    "\n",
    "# Training a neural network\n",
    "# Input data\n",
    "# Define the model architecture\n",
    "# Fit / train the model\n",
    "# Test the output\n",
    "\n",
    "# Training basically means how to minimize the cost functions by adjusting the parameters\n",
    "# Specify how to compute the output, Specify the loss and cost, Train to minimize the cost\n",
    "# Back propagation top compute partial derivative terms\n",
    "\n",
    "# Activation functions\n",
    "# Linear activation function\n",
    "# Sigmoind functions\n",
    "# ReLU - Rectified Linear unit\n",
    "\n",
    "# How to choose activation function\n",
    "# Binary classification problem - sigmoid as output layer\n",
    "# Regression problem - Linear activation function y = +/-\n",
    "# y >= 0 then use ReLU\n",
    "\n",
    "# Learning is faster in ReLU compare to sigmoid function due to flat edges\n",
    "# For hidder layer use ReLU activation method\n",
    "# There are more activation function\n",
    "# why ? \n",
    "# Using Linear regression as hidden and output layer makes model linear\n",
    "\n",
    "# Multiclass classification function\n",
    "# Generalization of logistics function\n",
    "# Softmax regression algorithm | Softmax Activation function\n",
    "# Cost function of softmax regression\n",
    "# Softmax output layer for multiple class classification\n",
    "# Trainig the different types of neural networks\n",
    "\n",
    "# Removing the numerical Roundoff errors\n",
    "# Multi-label classification problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Neural Network / Advanced optimizations\n",
    "- There are better function for cost than gradient descent\n",
    "- Adam algorithm - it increase or decrease the learning rate based on the steps that is not the case in gradient descent\n",
    "- Adaptive Moment estimation - Adam\n",
    "- MNIST Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different type of layer\n",
    "# Dense layer\n",
    "# Convolution Layer\n",
    "# faster, need less data and less prone to overfitting\n",
    "# Convolutional Neural Network\n",
    "# Each unit looks at the certain window that makes it more faster and less error prone\n",
    "# So research is still open to finding new architecture of layers and activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back Propagation \n",
    "- sympy library for derivative\n",
    "- Computation graph\n",
    "- Computing the derivative will be right to left that's why it is called backward Propagation\n",
    "- Chain rule for calculus\n",
    "- Back prop is efficient way to compute derivatives\n",
    "- Reduce calculation from N * P to N + p for derivatives\n",
    "- Auto diff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advice for applying machine learning\n",
    "- How to evaluate and improve the ML algorithm\n",
    "- How to evaluate the model\n",
    "- 70% training and 30% testing data\n",
    "- Computing testing error and computing training error\n",
    "- Model selection - choosing right model\n",
    "- Training | 60 / Cross Validation, Development set, dev set | 20/ Test set | 20\n",
    "- Train on diff model then evaluate on validations set whoever have less error\n",
    "- choosing how many layers and units in the layer same procedure we can follow\n",
    "- Choosing parameter or choosing an architechture\n",
    "- How to diagnost your model\n",
    "- Diagnosing bias and variance\n",
    "-  Bias / variance\n",
    "- High bias (underfit) j_train wii be high (j_trian ~ j_cv) - Not doing great on training set\n",
    "- High variance (overfit) - j_cv >> j_train (j_train may be low) - Not doing great on cross validation set\n",
    "- High bias and high variance j_train will be high and j_cv >> j_train - Not doing great on both\n",
    " \n",
    "- Regularization and bias/variance\n",
    "- Choosing the regularization parameter lambda\n",
    "- Establishing a baseline level of performance\n",
    "- Human level performance\n",
    "- Competing algorithms performance \n",
    "- Experinence\n",
    "- Baseline performance | Training error | Cross validation error\n",
    "- Learning curve\n",
    "- High bias will not resolve by more training data think on changing the algo\n",
    "- High variance - Increasing the training data will work or might help\n",
    "- Get more training example\n",
    "- Try smaller sets of features\n",
    "- Try getting additional features\n",
    "- Try adding polynomial features (x, x^2)\n",
    "- Try decreasing lambda\n",
    "- Try increasing lambda\n",
    "- Bias and variance takes short time to learn but long time to master\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias and Variance in neural networks\n",
    "Formula\n",
    "- Does it do well on the training set? - No -  bigger network/ better architechture\n",
    "- Yes - Does it do well on the cross validation set? - No - More data and repeat the first step\n",
    "- Yes - Done\n",
    "- large neural network doesn't bias but it does get expensive to train but gives better result\n",
    "\n",
    "Machine learning development process\n",
    "- choose architecture(model, data, etc) -> Train model -> Diagnostics (bias, variance and error analysis)\n",
    "\n",
    "Error Analysis\n",
    "- Manually group the failed results and oberve why it failed\n",
    "\n",
    "More Data\n",
    "- Data augmentation\n",
    "- Data Synthesis\n",
    "- AI = Code + Data\n",
    "- Model centric approach and Data centric approach\n",
    "\n",
    "Transfer learning for different data\n",
    "- Training on smaller set will gives model headstart that we can use to generalize the bigger task in that area\n",
    "- Supervised pretraining > Fine tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full cycle of machine learning project\n",
    "- Scope project -> collect data <-> Train Model (Training error analysis & Iterative improvement) -> Deploy in production\n",
    "- Ensure reliable and efficient predictions\n",
    "- Scaling\n",
    "- Logging\n",
    "- System monitoring\n",
    "- Model updates\n",
    "\n",
    "#### Fairness, bias, and ethics\n",
    "- Scope project, collect data, Train model, Audit, Deploy in production\n",
    "\n",
    "#### Skewed datasets\n",
    "- Error Metrics - Precision/recall\n",
    "- Trade off precision and recall\n",
    "- F1 score - Harmonic mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
