{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes from course\n",
    "#### Decision Tree\n",
    "- Tree, Make decision based on nodes and leaf node makes prediction\n",
    "\n",
    "Learning Process\n",
    "- How to choose features on node level - Maximize purity\n",
    "- when to stop spillting\n",
    "- Define maximum depth, Limit the size of tree\n",
    "- Use threshold to stop branching \n",
    "\n",
    "\n",
    "Entropy as a measure of impurity\n",
    "- Choose node which reduces the impurity the most\n",
    "- Reduction in entropy called information gain\n",
    "- weighted average entropy\n",
    "- Reduction in entropy | Information gain\n",
    " \n",
    "Decision Tree Learning\n",
    "- Start wit all example at the root nodec\n",
    "- calculate information gain for all possible features, and pick the one with the highest information gain\n",
    "- Split dataset according to the selected feature and create left and right branches of tree\n",
    "- Keep repeating splitting process until stopping criteria is met:\n",
    "    - when a node is 100% one class\n",
    "    - when spilitting a node will result in the tree exceeding a maximum depth\n",
    "    - Information grain from additional split is less than threshold\n",
    "    - when number of examples in a node is below a threshold\n",
    "\n",
    "- One hot encoding\n",
    "- continuous features\n",
    "\n",
    "Regression with Decision Trees\n",
    "- Largest reduction in variance to choosing a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7219280948873623"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = - (0.8 * np.log2(0.8)) - (0.2 * (np.log2(0.2)))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree ensembles - collection of multiple trees\n",
    "# Using multiple decision trees\n",
    "\n",
    "# Sampling with replacement\n",
    "# Random forest algorigthm\n",
    "#  Given training set of size m\n",
    "# for b = 1 to B:\n",
    "# Use sampling with replacement to create a new training set of size m Train a decision tree on the new dataset\n",
    "# Randomizing the feature choice\n",
    "\n",
    "# XGBoost - Boosted trees intuition\n",
    "# Given training set of size m\n",
    "# for b = 1 to B:\n",
    "# Use sampling with replacement to create a new training set of size m \n",
    "# But instead of picking from all examples with equal (1/m) probability, make it more likely to pick\n",
    "# misclassified examples from previously trained trees\n",
    "# Train a decision tree on the new dataset\n",
    "# XGBoost (extreme Gradient Boosting)\n",
    "\n",
    "# For Kaggle - XGBoost and Deep Learning (Neural network)\n",
    "\n",
    "# Decision Trees vs Neural Networks\n",
    "# Decision trees\n",
    "# Works well on structured data\n",
    "# Not recomended for unstructured data\n",
    "# Fast\n",
    "# small decision trees is easy to understand\n",
    "# Neural networks\n",
    "# works well on all types of data\n",
    "# slower than decision tree\n",
    "# works with transfer learning\n",
    "# multimodal is easier \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
